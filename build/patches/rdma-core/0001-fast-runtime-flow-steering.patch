From 34e4a29d938493b65ad0069f67a8c308b0ef1edb Mon Sep 17 00:00:00 2001
From: Josh Fried <joshuafried@gmail.com>
Date: Mon, 24 Apr 2023 20:46:30 -0400
Subject: [PATCH 1/3] fast runtime flow steering

- support directly re-writing ICM address for existing forward-to-QP actions
---
 providers/mlx5/dr_domain.c |  7 ++++---
 providers/mlx5/dr_rule.c   | 33 +++++++++++++++++++++++++++++++++
 providers/mlx5/dr_send.c   | 29 +++++++++++++++++++++++++++--
 providers/mlx5/dr_ste.h    |  1 +
 providers/mlx5/dr_ste_v0.c | 10 ++++++++++
 providers/mlx5/dr_ste_v1.c |  8 ++++++++
 providers/mlx5/libmlx5.map |  3 +++
 providers/mlx5/mlx5dv.h    |  7 +++++++
 providers/mlx5/mlx5dv_dr.h |  3 +++
 9 files changed, 96 insertions(+), 5 deletions(-)

diff --git a/providers/mlx5/dr_domain.c b/providers/mlx5/dr_domain.c
index 53d9decb..612969fe 100644
--- a/providers/mlx5/dr_domain.c
+++ b/providers/mlx5/dr_domain.c
@@ -307,9 +307,9 @@ static int dr_domain_caps_init(struct ibv_context *ctx,
 	/* Non FDB type is supported over root table or when we can enable
 	 * force-loopback.
 	 */
-	if ((dmn->type != MLX5DV_DR_DOMAIN_TYPE_FDB) &&
-	    !dr_send_allow_fl(&dmn->info.caps))
-		return 0;
+//	if ((dmn->type != MLX5DV_DR_DOMAIN_TYPE_FDB) &&
+//	    !dr_send_allow_fl(&dmn->info.caps))
+//		return 0;
 
 	ret = dr_domain_vports_init(dmn);
 	if (ret)
@@ -470,6 +470,7 @@ mlx5dv_dr_domain_create(struct ibv_context *ctx,
 	dmn->type = type;
 	atomic_init(&dmn->refcount, 1);
 	list_head_init(&dmn->tbl_list);
+	dmn->spinlock = 0;
 
 	ret = pthread_spin_init(&dmn->debug_lock, PTHREAD_PROCESS_PRIVATE);
 	if (ret) {
diff --git a/providers/mlx5/dr_rule.c b/providers/mlx5/dr_rule.c
index ccba6421..abcb53bd 100644
--- a/providers/mlx5/dr_rule.c
+++ b/providers/mlx5/dr_rule.c
@@ -32,8 +32,11 @@
 
 #include <stdlib.h>
 #include <ccan/minmax.h>
+#include "mlx5dv.h"
 #include "mlx5dv_dr.h"
 
+#include "dr_ste.h"
+
 /* +1 for the cross GVMI STE */
 #define DR_RULE_MAX_STE_CHAIN (DR_RULE_MAX_STES + DR_ACTION_MAX_STES + 1)
 
@@ -1630,6 +1633,36 @@ struct mlx5dv_dr_rule *mlx5dv_dr_rule_create(struct mlx5dv_dr_matcher *matcher,
 	return rule;
 }
 
+
+int switch_qp_action(struct mlx5dv_dr_rule *rule,
+	struct mlx5dv_dr_domain *dmn,
+	struct ibv_qp *nqp, struct ibv_qp *pqp)
+{
+	uint64_t old_qp_index;
+	struct dr_ste *ste = &rule->rx.nic_matcher->s_htbl->ste_arr[0];
+	struct mlx5_qp *next_qp = to_mqp(nqp);
+	struct mlx5_qp *prev_qp = to_mqp(pqp);
+	struct postsend_info send_info = {};
+
+	assert(dmn->spinlock == 1);
+
+	assert(ste->htbl->chunk->num_of_entries == 1);
+
+	old_qp_index = dmn->ste_ctx->get_hit_addr(ste->hw_ste) & ~0x1;
+	assert(old_qp_index == ((prev_qp->tir_icm_addr >> 5) & 0xffffffff));
+
+	dr_ste_set_hit_addr(dmn->ste_ctx, ste->hw_ste, next_qp->tir_icm_addr, 1);
+
+	send_info.write.addr    = (uintptr_t) ste->hw_ste;
+	send_info.write.length  = DR_STE_SIZE_REDUCED;
+	send_info.write.lkey    = 0;
+	send_info.remote_addr   = dr_ste_get_mr_addr(ste);
+	send_info.rkey          = ste->htbl->chunk->rkey;
+
+	return dr_postsend_icm_data_unlocked(dmn, &send_info, 0);
+}
+
+
 int mlx5dv_dr_rule_destroy(struct mlx5dv_dr_rule *rule)
 {
 	struct mlx5dv_dr_matcher *matcher = rule->matcher;
diff --git a/providers/mlx5/dr_send.c b/providers/mlx5/dr_send.c
index 16d69150..fa4155d5 100644
--- a/providers/mlx5/dr_send.c
+++ b/providers/mlx5/dr_send.c
@@ -39,7 +39,7 @@
 
 #define QUEUE_SIZE		128
 #define SIGNAL_PER_DIV_QUEUE	16
-#define TH_NUMS_TO_DRAIN	2
+#define TH_NUMS_TO_DRAIN	16
 
 enum {
 	CQ_OK = 0,
@@ -747,7 +747,7 @@ static void dr_fill_data_segs(struct mlx5dv_dr_domain *dmn,
 		dr_fill_write_args_segs(send_ring, send_info);
 }
 
-static int dr_postsend_icm_data(struct mlx5dv_dr_domain *dmn,
+int dr_postsend_icm_data_unlocked(struct mlx5dv_dr_domain *dmn,
 				struct postsend_info *send_info,
 				int ring_idx)
 {
@@ -768,6 +768,31 @@ out_unlock:
 	return ret;
 }
 
+void postsend_lock(struct mlx5dv_dr_domain *dmn)
+{
+	while (__sync_lock_test_and_set(&dmn->spinlock, 1)) {
+		while (dmn->spinlock)
+			asm volatile("pause");
+	}
+}
+
+void postsend_unlock(struct mlx5dv_dr_domain *dmn)
+{
+	__sync_lock_release(&dmn->spinlock);
+}
+
+static int dr_postsend_icm_data(struct mlx5dv_dr_domain *dmn,
+				struct postsend_info *send_info, int ring_idx)
+{
+	int ret;
+
+	postsend_lock(dmn);
+	ret = dr_postsend_icm_data_unlocked(dmn, send_info, ring_idx);
+	postsend_unlock(dmn);
+
+	return ret;
+}
+
 static int dr_get_tbl_copy_details(struct mlx5dv_dr_domain *dmn,
 				   struct dr_ste_htbl *htbl,
 				   uint8_t **data,
diff --git a/providers/mlx5/dr_ste.h b/providers/mlx5/dr_ste.h
index 832e2ca1..8b46beb5 100644
--- a/providers/mlx5/dr_ste.h
+++ b/providers/mlx5/dr_ste.h
@@ -208,6 +208,7 @@ struct dr_ste_ctx {
 	void (*set_miss_addr)(uint8_t *hw_ste_p, uint64_t miss_addr);
 	uint64_t (*get_miss_addr)(uint8_t *hw_ste_p);
 	void (*set_hit_addr)(uint8_t *hw_ste_p, uint64_t icm_addr, uint32_t ht_size);
+	uint64_t (*get_hit_addr)(uint8_t *hw_ste_p);
 	void (*set_byte_mask)(uint8_t *hw_ste_p, uint16_t byte_mask);
 	uint16_t (*get_byte_mask)(uint8_t *hw_ste_p);
 	void (*set_ctrl_always_hit_htbl)(uint8_t *hw_ste, uint16_t byte_mask,
diff --git a/providers/mlx5/dr_ste_v0.c b/providers/mlx5/dr_ste_v0.c
index b9a23ac0..af746fc9 100644
--- a/providers/mlx5/dr_ste_v0.c
+++ b/providers/mlx5/dr_ste_v0.c
@@ -341,6 +341,15 @@ static void dr_ste_v0_init_full(uint8_t *hw_ste_p, uint16_t lu_type,
 	DR_STE_SET(rx_steering_mult, hw_ste_p, miss_address_63_48, gvmi);
 }
 
+static uint64_t dr_ste_v0_get_hit_addr(uint8_t *hw_ste_p)
+{
+	uint64_t index = DR_STE_GET(general, hw_ste_p, next_table_base_31_5_size) |
+			DR_STE_GET(general, hw_ste_p, next_table_base_39_32_size) << 27;
+
+	return index;
+
+}
+
 static void dr_ste_v0_init(uint8_t *hw_ste_p, uint16_t lu_type,
 			   bool is_rx, uint16_t gvmi)
 {
@@ -1908,6 +1917,7 @@ static struct dr_ste_ctx ste_ctx_v0 = {
 	.set_miss_addr			= &dr_ste_v0_set_miss_addr,
 	.get_miss_addr			= &dr_ste_v0_get_miss_addr,
 	.set_hit_addr			= &dr_ste_v0_set_hit_addr,
+	.get_hit_addr			= &dr_ste_v0_get_hit_addr,
 	.set_byte_mask			= &dr_ste_v0_set_byte_mask,
 	.get_byte_mask			= &dr_ste_v0_get_byte_mask,
 	.set_ctrl_always_hit_htbl	= &dr_ste_v0_set_ctrl_always_hit_htbl,
diff --git a/providers/mlx5/dr_ste_v1.c b/providers/mlx5/dr_ste_v1.c
index 0bc8b5d9..00efb0ce 100644
--- a/providers/mlx5/dr_ste_v1.c
+++ b/providers/mlx5/dr_ste_v1.c
@@ -391,6 +391,13 @@ static bool dr_ste_v1_is_match_ste(uint16_t lu_type)
 	return ((lu_type >> 8) == DR_STE_V1_TYPE_MATCH);
 }
 
+static uint64_t dr_ste_v1_get_hit_addr(uint8_t *hw_ste_p)
+{
+	uint64_t index = DR_STE_GET(match_bwc_v1, hw_ste_p, next_table_base_31_5_size) |
+			DR_STE_GET(match_bwc_v1, hw_ste_p, next_table_base_39_32_size) << 27;
+	return index;
+}
+
 static void dr_ste_v1_init(uint8_t *hw_ste_p, uint16_t lu_type,
 			   bool is_rx, uint16_t gvmi)
 {
@@ -3589,6 +3596,7 @@ static struct dr_ste_ctx ste_ctx_v1 = {
 	.set_miss_addr			= &dr_ste_v1_set_miss_addr,
 	.get_miss_addr			= &dr_ste_v1_get_miss_addr,
 	.set_hit_addr			= &dr_ste_v1_set_hit_addr,
+	.get_hit_addr 			= &dr_ste_v1_get_hit_addr,
 	.set_byte_mask			= &dr_ste_v1_set_byte_mask,
 	.get_byte_mask			= &dr_ste_v1_get_byte_mask,
 	.set_ctrl_always_hit_htbl	= &dr_ste_v1_set_ctrl_always_hit_htbl,
diff --git a/providers/mlx5/libmlx5.map b/providers/mlx5/libmlx5.map
index 8eeb261b..2f901201 100644
--- a/providers/mlx5/libmlx5.map
+++ b/providers/mlx5/libmlx5.map
@@ -105,6 +105,9 @@ MLX5_1.10 {
 		mlx5dv_dr_table_create;
 		mlx5dv_dr_table_destroy;
 		mlx5dv_qp_ex_from_ibv_qp_ex;
+		switch_qp_action;
+		postsend_lock;
+		postsend_unlock;
 } MLX5_1.9;
 
 MLX5_1.11 {
diff --git a/providers/mlx5/mlx5dv.h b/providers/mlx5/mlx5dv.h
index 163aab7b..ad084c86 100644
--- a/providers/mlx5/mlx5dv.h
+++ b/providers/mlx5/mlx5dv.h
@@ -1999,6 +1999,13 @@ mlx5dv_dr_rule_create(struct mlx5dv_dr_matcher *matcher,
 		      size_t num_actions,
 		      struct mlx5dv_dr_action *actions[]);
 
+
+void postsend_lock(struct mlx5dv_dr_domain *dmn);
+void postsend_unlock(struct mlx5dv_dr_domain *dmn);
+int switch_qp_action(struct mlx5dv_dr_rule *rule,
+	struct mlx5dv_dr_domain *dmn,
+	struct ibv_qp *nqp, struct ibv_qp *pqp);
+
 int mlx5dv_dr_rule_destroy(struct mlx5dv_dr_rule *rule);
 
 enum mlx5dv_dr_action_flags {
diff --git a/providers/mlx5/mlx5dv_dr.h b/providers/mlx5/mlx5dv_dr.h
index 7c7d0836..2cc8e4a1 100644
--- a/providers/mlx5/mlx5dv_dr.h
+++ b/providers/mlx5/mlx5dv_dr.h
@@ -1065,6 +1065,7 @@ struct mlx5dv_dr_domain {
 	struct dr_domain_info		info;
 	struct list_head		tbl_list;
 	uint32_t			flags;
+	int				spinlock;
 	/* protect debug lists of all tracked objects */
 	pthread_spinlock_t		debug_lock;
 	/* statistcs */
@@ -1672,6 +1673,8 @@ int dr_send_ring_alloc(struct mlx5dv_dr_domain *dmn);
 void dr_send_ring_free(struct mlx5dv_dr_domain *dmn);
 int dr_send_ring_force_drain(struct mlx5dv_dr_domain *dmn);
 bool dr_send_allow_fl(struct dr_devx_caps *caps);
+int dr_postsend_icm_data_unlocked(struct mlx5dv_dr_domain *dmn,
+				  struct postsend_info *send_info, int ring_idx);
 int dr_send_postsend_ste(struct mlx5dv_dr_domain *dmn, struct dr_ste *ste,
 			 uint8_t *data, uint16_t size, uint16_t offset,
 			 uint8_t ring_idx);
-- 
2.34.1

