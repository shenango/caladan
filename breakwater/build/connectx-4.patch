diff --git a/inc/iokernel/control.h b/inc/iokernel/control.h
index d057e06..a39a36e 100644
--- a/inc/iokernel/control.h
+++ b/inc/iokernel/control.h
@@ -42,6 +42,7 @@ struct congestion_info {
 enum {
 	HWQ_INVALID = 0,
 	HWQ_MLX5,
+	HWQ_MLX5_QSTEERING,
 	HWQ_SPDK_NVME,
 	NR_HWQ,
 };
diff --git a/iokernel/control.c b/iokernel/control.c
index 8743bed..1eeb6b8 100644
--- a/iokernel/control.c
+++ b/iokernel/control.c
@@ -77,6 +77,7 @@ static int control_init_hwq(struct shm_region *r,
 	h->parity_bit_mask = hs->parity_bit_mask;
 	h->hwq_type = hs->hwq_type;
 	h->enabled = true;
+	h->queue_steering = h->hwq_type == HWQ_MLX5_QSTEERING;
 
 	if (!h->descriptor_table || !h->consumer_idx)
 		return -EINVAL;
diff --git a/iokernel/defs.h b/iokernel/defs.h
index 7ac71e0..75f5899 100644
--- a/iokernel/defs.h
+++ b/iokernel/defs.h
@@ -55,6 +55,7 @@ struct proc;
 
 struct hwq {
 	bool			enabled;
+	bool			queue_steering;
 	void			*descriptor_table;
 	uint32_t		*consumer_idx;
 	uint32_t		descriptor_log_size;
diff --git a/iokernel/sched.c b/iokernel/sched.c
index b9514f6..1037a87 100644
--- a/iokernel/sched.c
+++ b/iokernel/sched.c
@@ -257,7 +257,7 @@ static uint32_t hwq_find_head(struct hwq *h, uint32_t cur_tail, uint32_t last_he
 }
 
 static bool
-sched_measure_hardware_delay(struct thread *th, struct hwq *h,
+sched_measure_hardware_delay(struct proc *p, struct thread *th, struct hwq *h,
 			     bool update_pointers)
 {
 	uint32_t cur_tail, cur_head, last_head, last_tail;
@@ -287,8 +287,8 @@ sched_measure_hardware_delay(struct thread *th, struct hwq *h,
 	if (cur_tail != last_tail || h->busy_since == UINT64_MAX)
 		h->busy_since = cur_tsc;
 
-	return th->active ? wraps_lt(cur_tail, last_head) :
-				 cur_head != cur_tail;
+	return (th->active || (h->queue_steering && p->active_thread_count)) ?
+		wraps_lt(cur_tail, last_head) : cur_head != cur_tail;
 }
 
 static uint64_t calc_delay_tsc(uint64_t tsc)
@@ -297,7 +297,7 @@ static uint64_t calc_delay_tsc(uint64_t tsc)
 }
 
 static bool
-sched_measure_kthread_delay(struct thread *th,
+sched_measure_kthread_delay(struct proc *p, struct thread *th,
 			    uint64_t *rxq_tsc, uint64_t *uthread_tsc,
 			    uint64_t *storage_tsc, uint64_t *timer_tsc)
 {
@@ -358,12 +358,12 @@ sched_measure_kthread_delay(struct thread *th,
 	*timer_tsc = calc_delay_tsc(tmp);
 
 	/* DIRECTPATH: measure delay and update signals */
-	if (sched_measure_hardware_delay(th, &th->directpath_hwq, true))
+	if (sched_measure_hardware_delay(p, th, &th->directpath_hwq, true))
 		busy = true;
 	*rxq_tsc = MAX(*rxq_tsc, calc_delay_tsc(th->directpath_hwq.busy_since));
 
 	/* STORAGE: measure delay and update signals */
-	if (sched_measure_hardware_delay(th, &th->storage_hwq, true))
+	if (sched_measure_hardware_delay(p, th, &th->storage_hwq, true))
 		busy = true;
 	*storage_tsc = calc_delay_tsc(th->storage_hwq.busy_since);
 
@@ -393,7 +393,7 @@ static void sched_measure_delay(struct proc *p)
 	for (i = 0; i < p->thread_count; i++) {
 		uint64_t rxq_tsc, uthread_tsc, storage_tsc, timer_tsc;
 
-		busy |= sched_measure_kthread_delay(&p->threads[i],
+		busy |= sched_measure_kthread_delay(p, &p->threads[i],
 			&rxq_tsc, &uthread_tsc, &storage_tsc, &timer_tsc);
 		hdelay = MAX(rxq_tsc + uthread_tsc + storage_tsc + timer_tsc,
 			     hdelay);
@@ -421,7 +421,7 @@ static void sched_detect_io_for_idle_runtime(struct proc *p)
 	for (i = 0; i < p->thread_count; i++) {
 		th = &p->threads[i];
 
-		if (sched_measure_hardware_delay(th, &th->directpath_hwq, false)) {
+		if (sched_measure_hardware_delay(p, th, &th->directpath_hwq, false)) {
 			sched_add_core(p);
 			return;
 		}
diff --git a/runtime/cfg.c b/runtime/cfg.c
index 143f017..c801141 100644
--- a/runtime/cfg.c
+++ b/runtime/cfg.c
@@ -297,6 +297,7 @@ static int parse_enable_directpath(const char *name, const char *val)
 {
 #ifdef DIRECTPATH
 	cfg_directpath_enabled = true;
+	strncpy(directpath_arg, val, sizeof(directpath_arg));
 	return 0;
 #else
 	log_err("cfg: cannot enable directpath, "
diff --git a/runtime/defs.h b/runtime/defs.h
index b4b322d..6083e9a 100644
--- a/runtime/defs.h
+++ b/runtime/defs.h
@@ -548,6 +548,7 @@ struct net_driver_ops {
 	int (*register_flow)(unsigned int affininty, struct trans_entry *e, void **handle_out);
 	int (*deregister_flow)(struct trans_entry *e, void *handle);
 	uint32_t (*get_flow_affinity)(uint8_t ipproto, uint16_t local_port, struct netaddr remote);
+	int (*rxq_has_work)(struct hardware_q *rxq);
 };
 
 extern struct net_driver_ops net_ops;
@@ -555,11 +556,12 @@ extern struct net_driver_ops net_ops;
 #ifdef DIRECTPATH
 
 extern bool cfg_directpath_enabled;
+extern char directpath_arg[128];
 struct direct_txq {};
 
 static inline bool rx_pending(struct hardware_q *rxq)
 {
-	return cfg_directpath_enabled && hardware_q_pending(rxq);
+	return cfg_directpath_enabled && net_ops.rxq_has_work(rxq);
 }
 
 extern size_t directpath_rx_buf_pool_sz(unsigned int nrqs);
diff --git a/runtime/net/core.c b/runtime/net/core.c
index 3dc2598..bfb1747 100644
--- a/runtime/net/core.c
+++ b/runtime/net/core.c
@@ -610,6 +610,11 @@ static int deregister_flow_iokernel(struct trans_entry *e, void *handle)
 	return 0;
 }
 
+static int iokernel_has_work(struct hardware_q *rxq)
+{
+	return 0;
+}
+
 static struct net_driver_ops iokernel_ops = {
 	.rx_batch = rx_batch_iokernel,
 	.tx_single = net_tx_iokernel,
@@ -617,6 +622,7 @@ static struct net_driver_ops iokernel_ops = {
 	.register_flow =  register_flow_iokernel,
 	.deregister_flow = deregister_flow_iokernel,
 	.get_flow_affinity = compute_flow_affinity,
+	.rxq_has_work = iokernel_has_work,
 };
 
 /**
diff --git a/runtime/net/directpath/common.c b/runtime/net/directpath/common.c
index 0beaa40..a4e0bc2 100644
--- a/runtime/net/directpath/common.c
+++ b/runtime/net/directpath/common.c
@@ -2,7 +2,7 @@
 #include <base/kref.h>
 #include <base/mempool.h>
 #include <runtime/sync.h>
-
+#include <base/log.h>
 #include "defs.h"
 
 #ifdef DIRECTPATH
@@ -15,6 +15,14 @@ struct tcache *directpath_buf_tcache;
 DEFINE_PERTHREAD(struct tcache_perthread, directpath_buf_pt);
 
 bool cfg_directpath_enabled;
+char directpath_arg[128];
+
+enum {
+	RX_MODE_FLOW_STEERING = 0,
+	RX_MODE_QUEUE_STEERING,
+};
+
+int directpath_mode;
 
 size_t directpath_rx_buf_pool_sz(unsigned int nrqs)
 {
@@ -65,12 +73,27 @@ int directpath_init(void)
 	if (ret)
 		return ret;
 
-	/* initialize mlx5 */
-	ret = mlx5_init(rxq_out, txq_out, maxks, maxks);
-	if (ret)
-		return ret;
+	if (strncmp("qs", directpath_arg, 2) != 0) {
+		directpath_mode = RX_MODE_FLOW_STEERING;
+		ret = mlx5_init_flow_steering(rxq_out, txq_out, maxks, maxks);
+		if (ret == 0) {
+			log_err("directpath_init: selected flow steering mode");
+			return 0;
+		}
+	}
 
-	return 0;
+	if (strncmp("fs", directpath_arg, 2) != 0) {
+		directpath_mode = RX_MODE_QUEUE_STEERING;
+		ret = mlx5_init_queue_steering(rxq_out, txq_out, maxks, maxks);
+		if (ret == 0) {
+			log_err("directpath_init: selected queue steering mode");
+			return 0;
+		}
+	}
+
+	log_err("Could not intialize directpath, ret = %d", ret);
+
+	return ret ? ret : -EINVAL;
 
 }
 
@@ -92,7 +115,7 @@ int directpath_init_thread(void)
 		rxq->descriptor_table, (1 << hs->descriptor_log_size) * hs->nr_descriptors);
 	hs->parity_byte_offset = rxq->parity_byte_offset;
 	hs->parity_bit_mask = rxq->parity_bit_mask;
-	hs->hwq_type = HWQ_MLX5;
+	hs->hwq_type = (directpath_mode == RX_MODE_FLOW_STEERING) ? HWQ_MLX5 : HWQ_MLX5_QSTEERING;
 	hs->consumer_idx = ptr_to_shmptr(&netcfg.tx_region, rxq->shadow_tail, sizeof(uint32_t));
 
 	k->directpath_rxq = rxq;
diff --git a/runtime/net/directpath/defs.h b/runtime/net/directpath/defs.h
index 0074ff7..82175bc 100644
--- a/runtime/net/directpath/defs.h
+++ b/runtime/net/directpath/defs.h
@@ -21,6 +21,7 @@ extern struct mempool directpath_buf_mp;
 extern struct tcache *directpath_buf_tcache;
 extern DEFINE_PERTHREAD(struct tcache_perthread, directpath_buf_pt);
 extern void directpath_rx_completion(struct mbuf *m);
-extern int mlx5_init(struct hardware_q **rxq_out,
-	    struct direct_txq **txq_out, unsigned int nr_rxq,
-	    unsigned int nr_txq);
+extern int mlx5_init_queue_steering(struct hardware_q **rxq_out,struct direct_txq **txq_out,
+	             unsigned int nr_rxq, unsigned int nr_txq);
+extern int mlx5_init_flow_steering(struct hardware_q **rxq_out,struct direct_txq **txq_out,
+	             unsigned int nr_rxq, unsigned int nr_txq);
\ No newline at end of file
diff --git a/runtime/net/directpath/mlx5/mlx5.h b/runtime/net/directpath/mlx5/mlx5.h
index 15cba2a..84a78b6 100644
--- a/runtime/net/directpath/mlx5/mlx5.h
+++ b/runtime/net/directpath/mlx5/mlx5.h
@@ -10,11 +10,18 @@
 
 #include "../defs.h"
 
+#define PORT_NUM 1 // TODO: make this dynamic
+#define MLX5_ETH_L2_INLINE_HEADER_SIZE 18
 
 struct mlx5_rxq {
 	/* handle for runtime */
 	struct hardware_q rxq;
 
+	/* queue steering mode */
+	struct rcu_hlist_head head;
+	struct rcu_hlist_node link;
+	spinlock_t lock;
+
 	uint32_t consumer_idx;
 
 	struct mlx5dv_cq rx_cq_dv;
@@ -56,14 +63,12 @@ struct mlx5_txq {
 
 extern struct mlx5_rxq rxqs[NCPU];
 extern struct ibv_context *context;
+extern struct ibv_pd *pd;
 
 extern int mlx5_transmit_one(struct mbuf *m);
 extern int mlx5_gather_rx(struct hardware_q *rxq, struct mbuf **ms, unsigned int budget);
-extern int mlx5_steer_flows(unsigned int *new_fg_assignment);
-extern int mlx5_register_flow(unsigned int affinity, struct trans_entry *e, void **handle_out);
-extern int mlx5_deregister_flow(struct trans_entry *e, void *handle);
-extern uint32_t mlx5_get_flow_affinity(uint8_t ipproto,
-			 uint16_t local_port, struct netaddr remote);
+extern int mlx5_common_init(struct hardware_q **rxq_out, struct direct_txq **txq_out,
+	             unsigned int nr_rxq, unsigned int nr_txq, bool use_rss);
 
 static inline unsigned int nr_inflight_tx(struct mlx5_txq *v)
 {
@@ -109,7 +114,3 @@ static inline uint32_t mlx5_get_rss_result(struct mlx5_cqe64 *cqe)
 {
 	return ntoh32(*((uint32_t *)cqe + 3));
 }
-
-extern struct net_driver_ops mlx5_ops;
-
-int mlx5_init_flows(int nr_rxq);
diff --git a/runtime/net/directpath/mlx5/mlx5_flow.c b/runtime/net/directpath/mlx5/mlx5_flow_steering.c
similarity index 90%
rename from runtime/net/directpath/mlx5/mlx5_flow.c
rename to runtime/net/directpath/mlx5/mlx5_flow_steering.c
index 3b0b644..1d08aa3 100644
--- a/runtime/net/directpath/mlx5/mlx5_flow.c
+++ b/runtime/net/directpath/mlx5/mlx5_flow_steering.c
@@ -277,7 +277,7 @@ static int mlx5_init_root_table(void)
 	return 0;
 }
 
-int mlx5_register_flow(unsigned int affinity, struct trans_entry *e, void **handle_out)
+static int mlx5_register_flow(unsigned int affinity, struct trans_entry *e, void **handle_out)
 {
 	union match key = {0};
 
@@ -330,7 +330,7 @@ int mlx5_register_flow(unsigned int affinity, struct trans_entry *e, void **hand
 	return 0;
 }
 
-int mlx5_deregister_flow(struct trans_entry *e, void *handle)
+static int mlx5_deregister_flow(struct trans_entry *e, void *handle)
 {
 	int ret;
 
@@ -348,7 +348,7 @@ int mlx5_deregister_flow(struct trans_entry *e, void *handle)
 	return ret;
 }
 
-int mlx5_steer_flows(unsigned int *new_fg_assignment)
+static int mlx5_steer_flows(unsigned int *new_fg_assignment)
 {
 	int i, ret = 0;
 	struct tbl *tbl;
@@ -379,7 +379,7 @@ int mlx5_steer_flows(unsigned int *new_fg_assignment)
 
 }
 
-uint32_t mlx5_get_flow_affinity(uint8_t ipproto, uint16_t local_port, struct netaddr remote)
+static uint32_t mlx5_get_flow_affinity(uint8_t ipproto, uint16_t local_port, struct netaddr remote)
 {
 	bitmap_ptr_t map = ipproto == IPPROTO_TCP ? tcp_listen_ports :
 			  udp_listen_ports;
@@ -390,7 +390,7 @@ uint32_t mlx5_get_flow_affinity(uint8_t ipproto, uint16_t local_port, struct net
 		return (local_port & PORT_MASK) % maxks;
 }
 
-int mlx5_init_flows(int rxq_count)
+static int mlx5_init_flows(int rxq_count)
 {
 	int ret;
 
@@ -431,4 +431,39 @@ int mlx5_init_flows(int rxq_count)
 
 }
 
+
+static int mlx5_fs_have_work(struct hardware_q *rxq)
+{
+	return hardware_q_pending(rxq);
+}
+
+static struct net_driver_ops mlx5_net_ops_flow_steering = {
+	.rx_batch = mlx5_gather_rx,
+	.tx_single = mlx5_transmit_one,
+	.steer_flows = mlx5_steer_flows,
+	.register_flow = mlx5_register_flow,
+	.deregister_flow = mlx5_deregister_flow,
+	.get_flow_affinity = mlx5_get_flow_affinity,
+	.rxq_has_work = mlx5_fs_have_work,
+};
+
+
+int mlx5_init_flow_steering(struct hardware_q **rxq_out,struct direct_txq **txq_out,
+	             unsigned int nr_rxq, unsigned int nr_txq)
+{
+	int ret;
+
+	ret = mlx5_common_init(rxq_out, txq_out, nr_rxq, nr_txq, false);
+	if (ret)
+		return ret;
+
+	ret = mlx5_init_flows(nr_rxq);
+	if (ret)
+		return ret;
+
+	net_ops = mlx5_net_ops_flow_steering;
+
+	return 0;
+}
+
 #endif
diff --git a/runtime/net/directpath/mlx5/mlx5_init.c b/runtime/net/directpath/mlx5/mlx5_init.c
index 0e77a19..e344f3f 100644
--- a/runtime/net/directpath/mlx5/mlx5_init.c
+++ b/runtime/net/directpath/mlx5/mlx5_init.c
@@ -22,8 +22,6 @@
 #include "mlx5.h"
 #include "mlx5_ifc.h"
 
-#define PORT_NUM 1 // TODO: make this dynamic
-
 static struct mlx5_txq txqs[NCPU];
 
 struct mlx5_rxq rxqs[NCPU];
@@ -43,10 +41,10 @@ static void mlx5_init_tx_segment(struct mlx5_txq *v, unsigned int idx)
 	segment = v->tx_qp_dv.sq.buf + idx * v->tx_qp_dv.sq.stride;
 	ctrl = segment;
 	eseg = segment + sizeof(*ctrl);
-	dpseg = (void *)eseg + (offsetof(struct mlx5_wqe_eth_seg, inline_hdr) & ~0xf);
+	dpseg = (void *)eseg + ((offsetof(struct mlx5_wqe_eth_seg, inline_hdr) + MLX5_ETH_L2_INLINE_HEADER_SIZE) & ~0xf);
 
 	size = (sizeof(*ctrl) / 16) +
-	       (offsetof(struct mlx5_wqe_eth_seg, inline_hdr)) / 16 +
+	       ((offsetof(struct mlx5_wqe_eth_seg, inline_hdr)) + MLX5_ETH_L2_INLINE_HEADER_SIZE) / 16 +
 	       sizeof(struct mlx5_wqe_data_seg) / 16;
 
 	/* set ctrl segment */
@@ -58,6 +56,8 @@ static void mlx5_init_tx_segment(struct mlx5_txq *v, unsigned int idx)
 	/* set eseg */
 	memset(eseg, 0, sizeof(struct mlx5_wqe_eth_seg));
 	eseg->cs_flags |= MLX5_ETH_WQE_L3_CSUM | MLX5_ETH_WQE_L4_CSUM;
+	eseg->inline_hdr_sz = htobe16(MLX5_ETH_L2_INLINE_HEADER_SIZE);
+
 
 	/* set dpseg */
 	dpseg->lkey = htobe32(mr_tx->lkey);
@@ -78,7 +78,7 @@ static struct mlx5dv_ctx_allocators dv_allocators = {
 	.free = simple_free,
 };
 
-static int mlx5_create_rxq(int index, struct mlx5_rxq *v)
+static int mlx5_create_rxq(int index, struct mlx5_rxq *v, bool use_rss)
 {
 	int i, ret;
 	unsigned char *buf;
@@ -127,32 +127,35 @@ static int mlx5_create_rxq(int index, struct mlx5_rxq *v)
 	if (ret)
 		return -ret;
 
-	struct ibv_wq *ind_tbl[1] = {v->rx_wq};
-	struct ibv_rwq_ind_table_init_attr rwq_attr = {0};
-	rwq_attr.ind_tbl = ind_tbl;
-	v->rwq_ind_table = ibv_create_rwq_ind_table(context, &rwq_attr);
-	if (!v->rwq_ind_table)
-		return -errno;
-
-	static unsigned char rss_key[40];
-	struct ibv_rx_hash_conf rss_cnf = {
-		.rx_hash_function = IBV_RX_HASH_FUNC_TOEPLITZ,
-		.rx_hash_key_len = ARRAY_SIZE(rss_key),
-		.rx_hash_key = rss_key,
-		.rx_hash_fields_mask = IBV_RX_HASH_SRC_IPV4 | IBV_RX_HASH_DST_IPV4 | IBV_RX_HASH_SRC_PORT_TCP | IBV_RX_HASH_DST_PORT_TCP,
-	};
-
-	struct ibv_qp_init_attr_ex qp_ex_attr = {
-		.qp_type = IBV_QPT_RAW_PACKET,
-		.comp_mask = IBV_QP_INIT_ATTR_RX_HASH | IBV_QP_INIT_ATTR_IND_TABLE | IBV_QP_INIT_ATTR_PD,
-		.pd = pd,
-		.rwq_ind_tbl = v->rwq_ind_table,
-		.rx_hash_conf = rss_cnf,
-	};
-
-	v->qp = ibv_create_qp_ex(context, &qp_ex_attr);
-	if (!v->qp)
-		return -errno;
+	/* Create 1 QP per WQ if not using RSS */
+	if (!use_rss) {
+		struct ibv_wq *ind_tbl[1] = {v->rx_wq};
+		struct ibv_rwq_ind_table_init_attr rwq_attr = {0};
+		rwq_attr.ind_tbl = ind_tbl;
+		v->rwq_ind_table = ibv_create_rwq_ind_table(context, &rwq_attr);
+		if (!v->rwq_ind_table)
+			return -errno;
+
+		static unsigned char null_rss[40];
+		struct ibv_rx_hash_conf rss_cnf = {
+				.rx_hash_function = IBV_RX_HASH_FUNC_TOEPLITZ,
+				.rx_hash_key_len = ARRAY_SIZE(null_rss),
+				.rx_hash_key = null_rss,
+				.rx_hash_fields_mask = IBV_RX_HASH_SRC_IPV4 | IBV_RX_HASH_DST_IPV4 | IBV_RX_HASH_SRC_PORT_TCP | IBV_RX_HASH_DST_PORT_TCP,
+			};
+
+		struct ibv_qp_init_attr_ex qp_ex_attr = {
+			.qp_type = IBV_QPT_RAW_PACKET,
+			.comp_mask = IBV_QP_INIT_ATTR_RX_HASH | IBV_QP_INIT_ATTR_IND_TABLE | IBV_QP_INIT_ATTR_PD,
+			.pd = pd,
+			.rwq_ind_tbl = v->rwq_ind_table,
+			.rx_hash_conf = rss_cnf,
+		};
+
+		v->qp = ibv_create_qp_ex(context, &qp_ex_attr);
+		if (!v->qp)
+			return -errno;
+	}
 
 	/* expose direct verbs objects */
 	struct mlx5dv_obj obj = {
@@ -244,7 +247,7 @@ static int mlx5_init_txq(int index, struct mlx5_txq *v)
 			.max_send_wr = SQ_NUM_DESC,
 			.max_recv_wr = 0,
 			.max_send_sge = 1,
-			.max_inline_data = 0, // TODO: should inline some data?
+			.max_inline_data = 0,
 		},
 		.qp_type = IBV_QPT_RAW_PACKET,
 		.sq_sig_all = 1,
@@ -262,7 +265,7 @@ static int mlx5_init_txq(int index, struct mlx5_txq *v)
 	struct ibv_qp_attr qp_attr;
 	memset(&qp_attr, 0, sizeof(qp_attr));
 	qp_attr.qp_state = IBV_QPS_INIT;
-	qp_attr.port_num = 1;
+	qp_attr.port_num = PORT_NUM;
 	ret = ibv_modify_qp(v->tx_qp, &qp_attr, IBV_QP_STATE | IBV_QP_PORT);
 	if (ret)
 		return -ret;
@@ -309,20 +312,11 @@ static int mlx5_init_txq(int index, struct mlx5_txq *v)
 	return 0;
 }
 
-static struct net_driver_ops mlx5_net_ops = {
-	.rx_batch = mlx5_gather_rx,
-	.tx_single = mlx5_transmit_one,
-	.steer_flows = mlx5_steer_flows,
-	.register_flow = mlx5_register_flow,
-	.deregister_flow = mlx5_deregister_flow,
-	.get_flow_affinity = mlx5_get_flow_affinity,
-};
-
 /*
  * mlx5_init - intialize all TX/RX queues
  */
-int mlx5_init(struct hardware_q **rxq_out, struct direct_txq **txq_out,
-	             unsigned int nr_rxq, unsigned int nr_txq)
+int mlx5_common_init(struct hardware_q **rxq_out, struct direct_txq **txq_out,
+	             unsigned int nr_rxq, unsigned int nr_txq, bool use_rss)
 {
 	int i, ret;
 
@@ -351,7 +345,7 @@ int mlx5_init(struct hardware_q **rxq_out, struct direct_txq **txq_out,
 		return -1;
 	}
 
-	attr.flags = MLX5DV_CONTEXT_FLAGS_DEVX;
+	attr.flags = use_rss ? 0 : MLX5DV_CONTEXT_FLAGS_DEVX;
 	context = mlx5dv_open_device(ib_dev, &attr);
 	if (!context) {
 		log_err("mlx5_init: Couldn't get context for %s (errno %d)",
@@ -388,17 +382,13 @@ int mlx5_init(struct hardware_q **rxq_out, struct direct_txq **txq_out,
 	}
 
 	for (i = 0; i < nr_rxq; i++) {
-		ret = mlx5_create_rxq(i, &rxqs[i]);
+		ret = mlx5_create_rxq(i, &rxqs[i], use_rss);
 		if (ret)
 			return ret;
 
 		rxq_out[i] = &rxqs[i].rxq;
 	}
 
-	ret = mlx5_init_flows(nr_rxq);
-	if (ret)
-		return ret;
-
 	for (i = 0; i < nr_txq; i++) {
 		ret = mlx5_init_txq(i, &txqs[i]);
 		if (ret)
@@ -407,8 +397,6 @@ int mlx5_init(struct hardware_q **rxq_out, struct direct_txq **txq_out,
 		txq_out[i] = &txqs[i].txq;
 	}
 
-	net_ops = mlx5_net_ops;
-
 	return 0;
 }
 
diff --git a/runtime/net/directpath/mlx5/mlx5_queue_steering.c b/runtime/net/directpath/mlx5/mlx5_queue_steering.c
new file mode 100644
index 0000000..17fcea6
--- /dev/null
+++ b/runtime/net/directpath/mlx5/mlx5_queue_steering.c
@@ -0,0 +1,270 @@
+
+#ifdef DIRECTPATH
+
+#include <util/mmio.h>
+#include <util/udma_barrier.h>
+
+#include "../../defs.h"
+#include "mlx5.h"
+#include "mlx5_ifc.h"
+
+static unsigned char rss_key[40] = {
+	0x82, 0x19, 0xFA, 0x80, 0xA4, 0x31, 0x06, 0x59, 0x3E, 0x3F, 0x9A,
+	0xAC, 0x3D, 0xAE, 0xD6, 0xD9, 0xF5, 0xFC, 0x0C, 0x63, 0x94, 0xBF,
+	0x8F, 0xDE, 0xD2, 0xC5, 0xE2, 0x04, 0xB1, 0xCF, 0xB1, 0xB1, 0xA1,
+	0x0D, 0x6D, 0x86, 0xBA, 0x61, 0x78, 0xEB};
+
+static unsigned int nr_rxq;
+static unsigned int queue_assignments[NCPU];
+
+static int mlx5_qs_init_flows(void)
+{
+	int i;
+	struct ibv_wq *ind_tbl[2048];
+	struct ibv_flow *eth_flow;
+	struct ibv_qp *tcp_qp, *other_qp;
+	struct ibv_rwq_ind_table *rwq_ind_table;
+
+	for (i = 0; i < 2048; i++)
+		ind_tbl[i] = rxqs[i % nr_rxq].rx_wq;
+
+	/* Create Receive Work Queue Indirection Table */
+	struct ibv_rwq_ind_table_init_attr rwq_attr = {
+		.log_ind_tbl_size = __builtin_ctz(2048),
+		.ind_tbl = ind_tbl,
+		.comp_mask = 0,
+	};
+	rwq_ind_table = ibv_create_rwq_ind_table(context, &rwq_attr);
+
+	if (!rwq_ind_table)
+		return -errno;
+
+	/* Create the main RX QP using the indirection table */
+	struct ibv_rx_hash_conf rss_cnf = {
+		.rx_hash_function = IBV_RX_HASH_FUNC_TOEPLITZ,
+		.rx_hash_key_len = ARRAY_SIZE(rss_key),
+		.rx_hash_key = rss_key,
+		.rx_hash_fields_mask = IBV_RX_HASH_SRC_IPV4 | IBV_RX_HASH_DST_IPV4 | IBV_RX_HASH_SRC_PORT_TCP | IBV_RX_HASH_DST_PORT_TCP,
+	};
+
+	struct ibv_qp_init_attr_ex qp_ex_attr = {
+		.qp_type = IBV_QPT_RAW_PACKET,
+		.comp_mask =  IBV_QP_INIT_ATTR_IND_TABLE | IBV_QP_INIT_ATTR_RX_HASH | IBV_QP_INIT_ATTR_PD,
+		.pd = pd,
+		.rwq_ind_tbl = rwq_ind_table,
+		.rx_hash_conf = rss_cnf,
+	};
+
+	tcp_qp = ibv_create_qp_ex(context, &qp_ex_attr);
+	if (!tcp_qp)
+		return -errno;
+
+	rss_cnf.rx_hash_fields_mask = IBV_RX_HASH_SRC_IPV4 | IBV_RX_HASH_DST_IPV4 | IBV_RX_HASH_SRC_PORT_UDP | IBV_RX_HASH_DST_PORT_UDP,
+	qp_ex_attr.rx_hash_conf = rss_cnf;
+	other_qp = ibv_create_qp_ex(context, &qp_ex_attr);
+	if (!other_qp)
+		return -errno;
+
+	/* Route TCP packets for our MAC address to the QP with TCP RSS configuration */
+	struct raw_eth_flow_attr {
+		struct ibv_flow_attr attr;
+		struct ibv_flow_spec_eth spec_eth;
+		struct ibv_flow_spec_tcp_udp spec_tcp;
+	} __attribute__((packed)) flow_attr = {
+		.attr = {
+			.comp_mask = 0,
+			.type = IBV_FLOW_ATTR_NORMAL,
+			.size = sizeof(flow_attr),
+			.priority = 0,
+			.num_of_specs = 2,
+			.port = PORT_NUM,
+			.flags = 0,
+		},
+		.spec_eth = {
+			.type = IBV_FLOW_SPEC_ETH,
+			.size = sizeof(struct ibv_flow_spec_eth),
+			.val = {
+				.src_mac = {0x00, 0x00, 0x00, 0x00, 0x00, 0x00},
+				.ether_type = 0,
+				.vlan_tag = 0,
+			},
+			.mask = {
+				.dst_mac = {0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF},
+				.src_mac = {0x00, 0x00, 0x00, 0x00, 0x00, 0x00},
+				.ether_type = 0,
+				.vlan_tag = 0,
+			}
+		},
+		.spec_tcp = {
+			.type = IBV_FLOW_SPEC_TCP,
+			.size = sizeof(struct ibv_flow_spec_tcp_udp),
+			.val = {0},
+			.mask = {0},
+		},
+	};
+	memcpy(&flow_attr.spec_eth.val.dst_mac, netcfg.mac.addr, 6);
+	eth_flow = ibv_create_flow(tcp_qp, &flow_attr.attr);
+	if (!eth_flow)
+		return -errno;
+
+	/* Route other unicast packets to the QP with the UDP RSS configuration */
+	flow_attr.attr.num_of_specs = 1;
+	eth_flow = ibv_create_flow(other_qp, &flow_attr.attr);
+	if (!eth_flow)
+		return -errno;
+
+	/* Route broadcst packets to our set of RX work queues */
+	memset(&flow_attr.spec_eth.val.dst_mac, 0xff, 6);
+	eth_flow = ibv_create_flow(other_qp, &flow_attr.attr);
+	if (!eth_flow)
+		return -errno;
+
+	/* Route multicast traffic to our RX queues */
+	struct ibv_flow_attr mc_attr = {
+		.comp_mask = 0,
+		.type = IBV_FLOW_ATTR_MC_DEFAULT,
+		.size = sizeof(mc_attr),
+		.priority = 0,
+		.num_of_specs = 0,
+		.port = PORT_NUM,
+		.flags = 0,
+	};
+	eth_flow = ibv_create_flow(other_qp, &mc_attr);
+	if (!eth_flow)
+		return -errno;
+
+	return 0;
+}
+
+static uint32_t mlx5_qs_flow_affinity(uint8_t ipproto, uint16_t local_port, struct netaddr remote)
+{
+	uint32_t i, j, map, ret = 0, input_tuple[] = {
+		remote.ip, netcfg.addr, local_port | remote.port << 16
+	};
+
+	for (j = 0; j < ARRAY_SIZE(input_tuple); j++) {
+		for (map = input_tuple[j]; map; map &= (map - 1)) {
+			i = (uint32_t)__builtin_ctz(map);
+			ret ^= hton32(((const uint32_t *)rss_key)[j]) << (31 - i) |
+			       (uint32_t)((uint64_t)(hton32(((const uint32_t *)rss_key)[j + 1])) >>
+			       (i + 1));
+		}
+	}
+
+	return (ret % 2048) % (uint32_t)maxks;
+}
+
+static inline void assign_q(unsigned int qidx, unsigned int kidx)
+{
+	if (queue_assignments[qidx] == kidx)
+		return;
+
+	rcu_hlist_del(&rxqs[qidx].link);
+	rcu_hlist_add_head(&rxqs[kidx].head, &rxqs[qidx].link);
+	queue_assignments[qidx] = kidx;
+}
+
+static int mlx5_qs_init_qs(void)
+{
+	int i;
+
+	for (i = 0; i < nr_rxq; i++) {
+		rcu_hlist_init_head(&rxqs[i].head);
+		spin_lock_init(&rxqs[i].lock);
+		rcu_hlist_add_head(&rxqs[0].head, &rxqs[i].link);
+	}
+
+	return 0;
+}
+
+static int mlx5_qs_steer(unsigned int *new_fg_assignment)
+{
+	int i;
+	for (i = 0; i < nr_rxq; i++)
+		assign_q(i, new_fg_assignment[i]);
+
+	return 0;
+}
+
+static int mlx5_qs_gather_rx(struct hardware_q *rxq, struct mbuf **ms, unsigned int budget)
+{
+	struct hardware_q *hq;
+	struct mlx5_rxq *mrxq, *hrxq = container_of(rxq, struct mlx5_rxq, rxq);
+	struct rcu_hlist_node *node;
+
+	unsigned int pulled = 0;
+
+	rcu_hlist_for_each(&hrxq->head, node, !preempt_enabled()) {
+		mrxq = rcu_hlist_entry(node, struct mlx5_rxq, link);
+		hq = &mrxq->rxq;
+
+		if (hardware_q_pending(hq) && spin_try_lock(&mrxq->lock)) {
+			pulled += mlx5_gather_rx(hq, ms + pulled, budget - pulled);
+			spin_unlock(&mrxq->lock);
+			if (pulled == budget)
+				break;
+		}
+	}
+	return pulled;
+}
+
+static int mlx5_qs_register_flow(unsigned int affininty, struct trans_entry *e, void **handle_out)
+{
+	return 0;
+}
+
+static int mlx5_qs_deregister_flow(struct trans_entry *e, void *handle)
+{
+	return 0;
+}
+
+static int mlx5_qs_have_work(struct hardware_q *rxq)
+{
+	struct mlx5_rxq *mrxq, *hrxq = container_of(rxq, struct mlx5_rxq, rxq);
+	struct rcu_hlist_node *node;
+
+	rcu_hlist_for_each(&hrxq->head, node, !preempt_enabled()) {
+		mrxq = rcu_hlist_entry(node, struct mlx5_rxq, link);
+		if (hardware_q_pending(&mrxq->rxq))
+			return true;
+	}
+
+	return false;
+}
+
+
+static struct net_driver_ops mlx5_net_ops_queue_steering = {
+	.rx_batch = mlx5_qs_gather_rx,
+	.tx_single = mlx5_transmit_one,
+	.steer_flows = mlx5_qs_steer,
+	.register_flow = mlx5_qs_register_flow,
+	.deregister_flow = mlx5_qs_deregister_flow,
+	.get_flow_affinity = mlx5_qs_flow_affinity,
+	.rxq_has_work = mlx5_qs_have_work,
+};
+
+int mlx5_init_queue_steering(struct hardware_q **rxq_out, struct direct_txq **txq_out,
+	             unsigned int nrrxq, unsigned int nr_txq)
+{
+	int ret;
+
+	nr_rxq = nrrxq;
+
+	ret = mlx5_common_init(rxq_out, txq_out, nr_rxq, nr_txq, true);
+	if (ret)
+		return ret;
+
+	ret = mlx5_qs_init_flows();
+	if (ret)
+		return ret;
+
+	ret = mlx5_qs_init_qs();
+	if (ret)
+		return ret;
+
+	net_ops = mlx5_net_ops_queue_steering;
+
+	return 0;
+}
+
+#endif
diff --git a/runtime/net/directpath/mlx5/mlx5_rxtx.c b/runtime/net/directpath/mlx5/mlx5_rxtx.c
index a212985..52e0b44 100644
--- a/runtime/net/directpath/mlx5/mlx5_rxtx.c
+++ b/runtime/net/directpath/mlx5/mlx5_rxtx.c
@@ -110,14 +110,16 @@ int mlx5_transmit_one(struct mbuf *m)
 	segment = v->tx_qp_dv.sq.buf + (idx << v->tx_sq_log_stride);
 	ctrl = segment;
 	eseg = segment + sizeof(*ctrl);
-	dpseg = (void *)eseg + (offsetof(struct mlx5_wqe_eth_seg, inline_hdr) & ~0xf);
+	dpseg = (void *)eseg + ((offsetof(struct mlx5_wqe_eth_seg, inline_hdr) + MLX5_ETH_L2_INLINE_HEADER_SIZE) & ~0xf);
 
 	ctrl->opmod_idx_opcode = htobe32(((v->sq_head & 0xffff) << 8) |
 					       MLX5_OPCODE_SEND);
 
+	assert(mbuf_length(m) >= MLX5_ETH_L2_INLINE_HEADER_SIZE);
+	memcpy(eseg->inline_hdr_start, mbuf_data(m), MLX5_ETH_L2_INLINE_HEADER_SIZE);
 
-	dpseg->byte_count = htobe32(mbuf_length(m));
-	dpseg->addr = htobe64((uint64_t)mbuf_data(m));
+	dpseg->byte_count = htobe32(mbuf_length(m) - MLX5_ETH_L2_INLINE_HEADER_SIZE);
+	dpseg->addr = htobe64((uint64_t)mbuf_data(m) + MLX5_ETH_L2_INLINE_HEADER_SIZE);
 
 	/* record buffer */
 	store_release(&v->buffers[v->sq_head & (v->tx_qp_dv.sq.wqe_cnt - 1)], m);
